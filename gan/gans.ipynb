{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0.0, 1, (1000, 2))\n",
    "A = torch.tensor([[1, 2], [-0.1, 0.5]])\n",
    "b = torch.tensor([1, 2])\n",
    "data = torch.matmul(X, A) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(data[:100, 0].detach().numpy(), data[:100, 1].detach().numpy())\n",
    "print(f'The covariance matrix is\\n{torch.matmul(A.T, A)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "data_iter = d2l.load_array((data,), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_G = nn.Sequential(nn.Linear(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_D = nn.Sequential(\n",
    "    nn.Linear(2, 5), nn.Tanh(),\n",
    "    nn.Linear(5, 3), nn.Tanh(),\n",
    "    nn.Linear(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def update_D(X, Z, net_D, net_G, loss, trainer_D):\n",
    "    \"\"\"Update discriminator.\"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    ones = torch.ones((batch_size,), device=X.device)\n",
    "    zeros = torch.zeros((batch_size,), device=X.device)\n",
    "    trainer_D.zero_grad()\n",
    "    real_Y = net_D(X)\n",
    "    fake_X = net_G(Z)\n",
    "    # Do not need to compute gradient for `net_G`, detach it from\n",
    "    # computing gradients.\n",
    "    fake_Y = net_D(fake_X.detach())\n",
    "    loss_D = (loss(real_Y, ones.reshape(real_Y.shape)) +\n",
    "              loss(fake_Y, zeros.reshape(fake_Y.shape))) / 2\n",
    "    loss_D.backward()\n",
    "    trainer_D.step()\n",
    "    return loss_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def update_G(Z, net_D, net_G, loss, trainer_G):\n",
    "    \"\"\"Update generator.\"\"\"\n",
    "    batch_size = Z.shape[0]\n",
    "    ones = torch.ones((batch_size,), device=Z.device)\n",
    "    trainer_G.zero_grad()\n",
    "    # We could reuse `fake_X` from `update_D` to save computation\n",
    "    fake_X = net_G(Z)\n",
    "    # Recomputing `fake_Y` is needed since `net_D` is changed\n",
    "    fake_Y = net_D(fake_X)\n",
    "    loss_G = loss(fake_Y, ones.reshape(fake_Y.shape))\n",
    "    loss_G.backward()\n",
    "    trainer_G.step()\n",
    "    return loss_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\n",
    "    loss = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    for w in net_D.parameters():\n",
    "        nn.init.normal_(w, 0, 0.02)\n",
    "    for w in net_G.parameters():\n",
    "        nn.init.normal_(w, 0, 0.02)\n",
    "    trainer_D = torch.optim.Adam(net_D.parameters(), lr=lr_D)\n",
    "    trainer_G = torch.optim.Adam(net_G.parameters(), lr=lr_G)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
    "                            legend=['discriminator', 'generator'])\n",
    "    animator.fig.subplots_adjust(hspace=0.3)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train one epoch\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "        for (X,) in data_iter:\n",
    "            batch_size = X.shape[0]\n",
    "            Z = torch.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),\n",
    "                       update_G(Z, net_D, net_G, loss, trainer_G),\n",
    "                       batch_size)\n",
    "        # Visualize generated examples\n",
    "        Z = torch.normal(0, 1, size=(100, latent_dim))\n",
    "        fake_X = net_G(Z).detach().numpy()\n",
    "        animator.axes[1].cla()\n",
    "        animator.axes[1].scatter(data[:, 0], data[:, 1])\n",
    "        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n",
    "        animator.axes[1].legend(['real', 'generated'])\n",
    "        # Show the losses\n",
    "        loss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]\n",
    "        animator.add(epoch + 1, (loss_D, loss_G))\n",
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_D, lr_G, latent_dim, num_epochs = 0.05, 0.005, 2, 20\n",
    "train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\n",
    "      latent_dim, data[:100].detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tcc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc2b72fe97738a6d6f5a2660f6944b00a6e9a86f793f22d319c92b296a70f88b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
